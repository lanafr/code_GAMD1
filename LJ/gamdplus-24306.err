2023-10-23 22:26:12.030528: I external/xla/xla/service/service.cc:168] XLA service 0x8551e60 initialized for platform Interpreter (this does not guarantee that XLA will be used). Devices:
2023-10-23 22:26:12.030573: I external/xla/xla/service/service.cc:176]   StreamExecutor device (0): Interpreter, <undefined>
2023-10-23 22:26:12.038731: I external/xla/xla/pjrt/tfrt_cpu_pjrt_client.cc:218] TfrtCpuClient created.
2023-10-23 22:26:12.430050: I external/xla/xla/service/service.cc:168] XLA service 0x86a6210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-10-23 22:26:12.430135: I external/xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6
2023-10-23 22:26:12.431687: I external/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc:198] Using BFC allocator.
2023-10-23 22:26:12.433133: I external/xla/xla/pjrt/gpu/gpu_helpers.cc:105] XLA backend allocating 35730898944 bytes on device 0 for BFCAllocator.
2023-10-23 22:26:15.767727: I external/xla/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8500
2023-10-23 22:26:15.832318: I external/xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:52] Using nvlink for parallel linking
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
wandb: Currently logged in as: lana4655 (literallythehardestprojectever). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/guests/lana_frkin/GAMDplus/code/LJ/wandb/run-20231023_222627-zmxq11iy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-pond-113
wandb: ‚≠êÔ∏è View project at https://wandb.ai/literallythehardestprojectever/code-LJ
wandb: üöÄ View run at https://wandb.ai/literallythehardestprojectever/code-LJ/runs/zmxq11iy
Set SLURM handle signals.

  | Name       | Type           | Params
----------------------------------------------
0 | pnet_model | GNNAutoencoder | 2.1 M 
----------------------------------------------
2.1 M     Trainable params
42        Non-trainable params
2.1 M     Total params
8.205     Total estimated model params size (MB)
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: üöÄ View run zesty-pond-113 at: https://wandb.ai/literallythehardestprojectever/code-LJ/runs/zmxq11iy
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20231023_222627-zmxq11iy/logs
Traceback (most recent call last):
  File "train_autoencoder.py", line 287, in <module>
    main()
  File "train_autoencoder.py", line 283, in main
    train_model(args)
  File "train_autoencoder.py", line 262, in train_model
    trainer.fit(model)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 458, in fit
    self._run(model)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 756, in _run
    self.dispatch()
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 797, in dispatch
    self.accelerator.start_training(self)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 96, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 144, in start_training
    self._results = trainer.run_stage()
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 807, in run_stage
    return self.run_train()
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 842, in run_train
    self.run_sanity_check(self.lightning_module)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1107, in run_sanity_check
    self.run_evaluation()
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 962, in run_evaluation
    output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 174, in evaluation_step
    output = self.trainer.accelerator.validation_step(args)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 226, in validation_step
    return self.training_type_plugin.validation_step(*args)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 322, in validation_step
    return self.model(*args, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py", line 57, in forward
    output = self.module.validation_step(*inputs, **kwargs)
  File "train_autoencoder.py", line 181, in validation_step
    new_graph = self.do_the_autoencoding(old_graph
  File "train_autoencoder.py", line 105, in do_the_autoencoding
    graph_new = self.pnet_model(graph_old
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/../nn_module.py", line 566, in forward
    g_embed = self.gencoder(g.ndata['e'], g)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/../nn_module.py", line 555, in gencoder
    x = self.graph_conv1(h, g)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/../nn_module.py", line 224, in forward
    h = conv_layer.forward(graph, h) + h
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/../nn_module.py", line 155, in forward
    edge_code = self.edge_affine(g.edata['e'])
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/../nn_module.py", line 82, in forward
    return self.mlp_layer(feat)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guests/lana_frkin/GAMDplus/code/LJ/GAMDnew/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (6522x256 and 1x128)
2023-10-23 22:26:40.373427: I external/xla/xla/pjrt/tfrt_cpu_pjrt_client.cc:221] TfrtCpuClient destroyed.
